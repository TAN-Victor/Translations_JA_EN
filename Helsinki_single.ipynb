{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Machine Translation of News: Japanese to English Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental part - For single sentence examples (Can be run)\n",
    "\n",
    "As explained in the report, we couldn't apply postprocessing techniques globally, because errors were dependent on the sentence: some errors could occur for one sentence and not for another one with the same settings. Here are some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Victo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Victo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import evaluate\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "cache_dir = 'D:\\\\.cache'                               # CHANGE THIS TO YOUR OWN CACHE DIRECTORY, I didn't have enough space in my main disk\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def translate_text_print(model, tokenizer, input_text):\n",
    "\n",
    "    # Tokenize the input text, as Word level segmentation\n",
    "    tokens = tokenizer.tokenize(input_text)\n",
    "    print(\"Tokens:\", tokens)\n",
    "\n",
    "\n",
    "    # Tokenize the input text and convert it to tensors (IDs)\n",
    "        # Will both SEGMENT and ENCODE if is_split_into_words is False\n",
    "        # Will only ENCODE if is_split_into_words is True\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=50, is_split_into_words=False).to(device)\n",
    "    print(\"Input ids:\", inputs['input_ids'])\n",
    "    print(\"Input attention mask:\", inputs['attention_mask'])\n",
    "    print(\"Input with tokens:\", tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=False))\n",
    "\n",
    "\n",
    "    # Generate translation using model\n",
    "    translated = model.generate(**inputs)\n",
    "    print(\"Generated ids:\",  translated)\n",
    "\n",
    "\n",
    "    # Decode the output tokens to text\n",
    "    decoded_output = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    print(\"Decoded sentence:\", decoded_output)\n",
    "    return decoded_output\n",
    "\n",
    "def calculate_bleu_score(predictions, references, max_order=4):\n",
    "    bleu = evaluate.load(\"bleu\", cache_dir=cache_dir)\n",
    "    return bleu.compute(predictions=predictions, references=references, max_order=max_order)\n",
    "\n",
    "def calculate_chrf_score(predictions, references):\n",
    "    chrf = evaluate.load(\"chrf\", cache_dir=cache_dir)\n",
    "    return chrf.compute(predictions=predictions, references=references)\n",
    "\n",
    "def calculate_sacrebleu_score(predictions, references):\n",
    "    sacrebleu = evaluate.load(\"sacrebleu\", cache_dir=cache_dir)\n",
    "    return sacrebleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-ja-en', cache_dir=cache_dir, use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-ja-en', cache_dir=cache_dir).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test here: Change the Japanese sentence if you want a translation, change both if you want the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁私', 'は', '学生', 'です']\n",
      "Input ids: tensor([[  115,    18,  7323,    74,     0, 60715, 60715, 60715, 60715, 60715,\n",
      "         60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715,\n",
      "         60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715,\n",
      "         60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715,\n",
      "         60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715]],\n",
      "       device='cuda:0')\n",
      "Input attention mask: tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')\n",
      "Input with tokens: 私は学生です</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Generated ids: tensor([[60715,    23,    12,   111,    20,  3818,     2,     0]],\n",
      "       device='cuda:0')\n",
      "Decoded sentence: I'm a student.\n"
     ]
    }
   ],
   "source": [
    "z = {'en': 'I am a student', \n",
    "     'jp': '私は学生です'}\n",
    "translated = translate_text_print(model, tokenizer, z['jp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Dates\n",
    "\n",
    "Here, we can't find rules to convert \"2018: 7/14-8/19\" to \"2018年7月14日から8月19日\", even if the meaning still remains. However, because the structure of the date is not the same, BLEU score is 0 (because of ngrams > 3).\n",
    "\n",
    "Decoded sentence: It's August, by the time I see it! The duration of the event is from July 14th, 2018, to August 19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁見', '頃', 'は', '8', '月', 'が', 'オス', 'ス', 'メ', '!', '開', '催', '期間', 'は', '20', '18', '年', '7', '月', '14', '日', 'から', '8', '月', '19', '日', 'まで', '。']\n",
      "Input ids: tensor([[  390,  3072,    18,  1773,  1215,    34, 14722,   563,  4480,    40,\n",
      "          5613,  7601,  9668,    18,  2785,  7131,   494,  1784,  1215,  6994,\n",
      "           583,   135,  1773,  1215,  7240,   583,   417,  5832,     0, 60715,\n",
      "         60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715,\n",
      "         60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715]],\n",
      "       device='cuda:0')\n",
      "Input attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')\n",
      "Input with tokens: 見頃は8月がオススメ!開催期間は2018年7月14日から8月19日まで。</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Generated ids: tensor([[60715,   106,    12,    17,  5597,     3,    96,     5,   164,    23,\n",
      "           215,    36,    40,    66, 23514,    11,     5,  3811,    32,    85,\n",
      "          5402,   639,   736,     3,   490,  7131,     3,     8,  5597,  5310,\n",
      "             0]], device='cuda:0')\n",
      "Decoded sentence: It's August, by the time I see it! The duration of the event is from July 14th, 2018, to August 19.\n"
     ]
    }
   ],
   "source": [
    "z = {'en': 'The best season is August!Sunflower Festival in 2018: 7/14-8/19.',\n",
    " 'jp': '見頃は8月がオススメ!開催期間は2018年7月14日から8月19日まで。'} # 206\n",
    "translated = translate_text_print(model, tokenizer, z['jp'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Repetitive n-grams\n",
    "\n",
    "Here, the translated sentence has a very long chain of \"etc.\"\n",
    "\n",
    "\n",
    "Decoded sentence: I'm going to talk to you today about tourism, travel, vacations, entertainment, visits to friends and relatives, rest, therapy, reunions, social and service activities, etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁観光', '/', '旅行', '-', '旅行', '、', '休暇', '、', '娯', '楽', '、', '友人', 'や', '親族', 'の', '訪問', '、', '休', '養', '、', '治療', '、', '同', '窓', '会', 'や', '社', '交', '、', '奉仕', '活動', 'など', '、', '及び', '報酬', 'を', '伴', 'わ', 'ない', '音楽', 'や', 'スポーツ', 'など', 'イベント', '或', 'い', 'は', 'コンテスト', 'の', 'アマ', 'チュ', 'ア', '参加', 'ウ', '.']\n",
      "Input ids: tensor([[10926,   783,  8029,   146,  8029, 15168, 18751, 15168, 47596,  6129,\n",
      "         15168,  3767,   261, 41629,    13, 15487, 15168, 10958, 11009, 15168,\n",
      "          7077, 15168,  3626,  9452,  1486,   261,  4558,  5968, 15168, 19783,\n",
      "          5253,  1699, 15168, 22963, 10998,    22, 21294,   251,    72,  5658,\n",
      "           261, 16912,  1699, 13442, 16914,   125,    18, 41745,    13,     0]],\n",
      "       device='cuda:0')\n",
      "Input attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], device='cuda:0')\n",
      "Input with tokens: 観光/旅行-旅行、休暇、娯楽、友人や親族の訪問、休養、治療、同窓会や社交、奉仕活動など、及び報酬を伴わない音楽やスポーツなどイベント或いはコンテストの</s>\n",
      "Generated ids: tensor([[60715,    23,    12,   111,   254,     8,   644,     8,    27,   508,\n",
      "           124, 38372,     3,  3884,     3,  8236,    17,     3,  7383,     3,\n",
      "          9533,     8,  1050,    19,  5443,     3,  1455,     3, 11299,     3,\n",
      "         30750,    17,     3,  2455,    19,  1166,  3574,     3,  8445,     2,\n",
      "             3,  8445,     2,     3,  8445,     2,     3,  8445,     2,     3,\n",
      "          8445,     2,     3,  8445,     2,     3,  8445,     2,     3,  8445,\n",
      "             2,     3,  8445,     2,     3,  8445,     2,     3,  8445,     2,\n",
      "             3,  8445,     2,     3,  8445,     2,     3,  8445,     2,     3,\n",
      "          8445,     2,     3,  8445,     2,     3,  8445,     2,     3,  8445,\n",
      "             2,     3,  8445,     2,     3,  8445,     2,     3,  8445,     2,\n",
      "             3,  8445,     2,     3,  8445,     2,     3,  8445,     2,     3,\n",
      "          8445,     2,     3,  8445,     2,     3,  8445,     2,     0]],\n",
      "       device='cuda:0')\n",
      "Decoded sentence: I'm going to talk to you today about tourism, travel, vacations, entertainment, visits to friends and relatives, rest, therapy, reunions, social and service activities, etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc., etc.\n",
      "BLEU score: {'bleu': 0.03732736050550657, 'precisions': [0.22123893805309736, 0.05357142857142857, 0.018018018018018018, 0.00909090909090909], 'brevity_penalty': 1.0, 'length_ratio': 1.6865671641791045, 'translation_length': 113, 'reference_length': 67}\n",
      "CHRF score: {'score': 27.739272452223695, 'char_order': 6, 'word_order': 0, 'beta': 2}\n"
     ]
    }
   ],
   "source": [
    "z = {'en': 'Pleasure/Tourism- The purpose of your planned travel is recreational in nature, including tourism, vacation (holiday), amusement, visits with friends or relatives, rest, medical treatment, activities of a fraternal, social, or service nature, and participation by amateurs, who will receive no remuneration, in musical, sports and similar events or contests.',\n",
    " 'jp': '観光/旅行-旅行、休暇、娯楽、友人や親族の訪問、休養、治療、同窓会や社交、奉仕活動など、及び報酬を伴わない音楽やスポーツなどイベント或いはコンテストのアマチュア参加ウ.'} # 232\n",
    "translated = translate_text_print(model, tokenizer, z['jp'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: \"、\" punctuation\n",
    "\n",
    "Here, the model just can't translate the sentence, and provides a non-related sentence.\n",
    "\n",
    "Decoded sentence: It's one of the most remarkable spots in the history of the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁著名', '人が', 'お', '忍び', 'で', '来', '場', 'すること', 'も', '多く', '、', 'また', '、', '豪華', '寝', '台', '列車', '「', 'な', 'な', 'つ', '星', '」', 'の一部', 'の', 'ツアー', 'にも', '組み込', 'まれる', 'など', '、', 'いま', '改め', 'て', '注目', 'されている', 'スポット', 'です', '。']\n",
      "Input ids: tensor([[12251,  1443,   816, 40990,    53,   808,  2312,  1987,   109,  1446,\n",
      "         15168,   798, 15168, 52499,  4683,  5067, 12926, 18155,    93,    93,\n",
      "           556,  3352, 20056,  5456,    13, 30778,   698, 29972, 12939,  1699,\n",
      "         15168,   693, 37550,    81, 11677,  2601, 36224,    74,  5832,     0,\n",
      "         60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715]],\n",
      "       device='cuda:0')\n",
      "Input attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')\n",
      "Input with tokens: 著名人がお忍びで来場することも多く、また、豪華寝台列車「ななつ星」の一部のツアーにも組み込まれるなど、いま改めて注目されているスポットです。</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Generated ids: tensor([[60715,   106,    12,    17,   105,    11,     5,   412,  6560, 16991,\n",
      "            25,     5,  1477,    11,     5,   268,     2,     0]],\n",
      "       device='cuda:0')\n",
      "Decoded sentence: It's one of the most remarkable spots in the history of the world.\n",
      "BLEU score: {'bleu': 0.0, 'precisions': [0.5714285714285714, 0.15384615384615385, 0.08333333333333333, 0.0], 'brevity_penalty': 0.3951177613268873, 'length_ratio': 0.5185185185185185, 'translation_length': 14, 'reference_length': 27}\n",
      "CHRF score: {'score': 16.422263374535333, 'char_order': 6, 'word_order': 0, 'beta': 2}\n"
     ]
    }
   ],
   "source": [
    "z = {'en': \"Celebrities quietly frequent the museum as well. It's also gaining attention from being part of one of the tours on the Seven Stars Cruise Train.\",\n",
    " 'jp': '著名人がお忍びで来場することも多く、また、豪華寝台列車「ななつ星」の一部のツアーにも組み込まれるなど、いま改めて注目されているスポットです。'} # 236\n",
    "translated = translate_text_print(model, tokenizer, z['jp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we delete the last \"、\":\n",
    "\n",
    "Decoded sentence: It's a new feature to be noticed that well-known people often sneak in, as well as to be included on a tour of some of the \"The Star of the Night.\"\n",
    "\n",
    "The previous part of the sentence about the Named Entity \"Star\" something is now here. Though, BLEU score of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁著名', '人が', 'お', '忍び', 'で', '来', '場', 'すること', 'も', '多く', '、', 'また', '、', '豪華', '寝', '台', '列車', '「', 'な', 'な', 'つ', '星', '」', 'の一部', 'の', 'ツアー', 'にも', '組み込', 'まれる', 'など', '▁いま', '改め', 'て', '注目', 'されている', 'スポット', 'です', '。']\n",
      "Input ids: tensor([[12251,  1443,   816, 40990,    53,   808,  2312,  1987,   109,  1446,\n",
      "         15168,   798, 15168, 52499,  4683,  5067, 12926, 18155,    93,    93,\n",
      "           556,  3352, 20056,  5456,    13, 30778,   698, 29972, 12939,  1699,\n",
      "          4726, 37550,    81, 11677,  2601, 36224,    74,  5832,     0, 60715,\n",
      "         60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715]],\n",
      "       device='cuda:0')\n",
      "Input attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')\n",
      "Input with tokens: 著名人がお忍びで来場することも多く、また、豪華寝台列車「ななつ星」の一部のツアーにも組み込まれるなど いま改めて注目されているスポットです。</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Generated ids: tensor([[60715,   106,    12,    17,    20,   372,  6605,     8,    48,  5684,\n",
      "            30,   350,   146, 31454,   157,   754, 19724,    25,     3,    68,\n",
      "           350,    68,     8,    48,  4828,    54,    20,  9326,    11,   199,\n",
      "            11,     5,    14,   913,  7041,    11,     5, 12011,   181,     0]],\n",
      "       device='cuda:0')\n",
      "Decoded sentence: It's a new feature to be noticed that well-known people often sneak in, as well as to be included on a tour of some of the \"The Star of the Night.\"\n",
      "BLEU score: {'bleu': 0.0, 'precisions': [0.2571428571428571, 0.058823529411764705, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.2962962962962963, 'translation_length': 35, 'reference_length': 27}\n",
      "CHRF score: {'score': 24.580330339261565, 'char_order': 6, 'word_order': 0, 'beta': 2}\n"
     ]
    }
   ],
   "source": [
    "z = {'en': \"Celebrities quietly frequent the museum as well. It's also gaining attention from being part of one of the tours on the Seven Stars Cruise Train.\",\n",
    " 'jp': '著名人がお忍びで来場することも多く、また、豪華寝台列車「ななつ星」の一部のツアーにも組み込まれるなど いま改めて注目されているスポットです。'} # 236\n",
    "translated = translate_text_print(model, tokenizer, z['jp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Special characters\n",
    "\n",
    "Here, the model can't translate the \"€\" symbol.\n",
    "\n",
    "Decoded sentence: You have it in manual transmissions and trim styles... and it's up to 85000 points automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁あなた', 'は', 'マニュアル', 'トランス', 'ミッション', 'と', 'トリ', 'ム', 'スタイル', 'で', 'それ', 'を', '持', 'っている', '...', 'と', 'また', '、', '自動', '的に', '1985', '0', '€', 'に', 'より', 'ます', '。']\n",
      "Input ids: tensor([[  166,    18, 39322, 44013, 19252,    42,  9640,  1172, 13056,    53,\n",
      "           441,    22,   762,   479,    70,    42,   798, 15168,  7295,  1212,\n",
      "         55261,  1852, 56070,    29,   582,    95,  5832,     0, 60715, 60715,\n",
      "         60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715,\n",
      "         60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715]],\n",
      "       device='cuda:0')\n",
      "Input attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')\n",
      "Input with tokens: あなたはマニュアルトランスミッションとトリムスタイルでそれを持っている...とまた、自動的に19850€によります。</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Generated ids: tensor([[60715,    87,    56,    36,    25, 18064, 17517,    17,    19, 12951,\n",
      "           111, 16057,    70,    19,    36,    12,    17,   150,     8, 13485,\n",
      "          4965,  2957,  9896,     2,     0]], device='cuda:0')\n",
      "Decoded sentence: You have it in manual transmissions and trim styles... and it's up to 85000 points automatically.\n",
      "BLEU score: {'bleu': 0.26529518334824453, 'precisions': [0.6, 0.3157894736842105, 0.2222222222222222, 0.11764705882352941], 'brevity_penalty': 1.0, 'length_ratio': 1.0526315789473684, 'translation_length': 20, 'reference_length': 19}\n",
      "CHRF score: {'score': 58.28853733656334, 'char_order': 6, 'word_order': 0, 'beta': 2}\n"
     ]
    }
   ],
   "source": [
    "z = {'en': 'You have it in Style trim with manual transmission ... and Also automatically by 19,850 €.',\n",
    " 'jp': 'あなたはマニュアルトランスミッションとトリムスタイルでそれを持っている...とまた、自動的に19850€によります。'} # 385\n",
    "translated = translate_text_print(model, tokenizer, z['jp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we translate \"euros\" into \"ユーロ\", we get:\n",
    "\n",
    "Decoded sentence: You have it in manual transmission and trim style... and it is automatically made up of 19850 euros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁あなた', 'は', 'マニュアル', 'トランス', 'ミッション', 'と', 'トリ', 'ム', 'スタイル', 'で', 'それ', 'を', '持', 'っている', '...', 'と', 'また', '、', '自動', '的に', '▁1985', '0', 'ユーロ', '▁に', 'より', 'ます', '。']\n",
      "Input ids: tensor([[  166,    18, 39322, 44013, 19252,    42,  9640,  1172, 13056,    53,\n",
      "           441,    22,   762,   479,    70,    42,   798, 15168,  7295,  1212,\n",
      "          9081,  1852, 37306,     7,   582,    95,  5832,     0, 60715, 60715,\n",
      "         60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715,\n",
      "         60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715, 60715]],\n",
      "       device='cuda:0')\n",
      "Input attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]], device='cuda:0')\n",
      "Input with tokens: あなたはマニュアルトランスミッションとトリムスタイルでそれを持っている...とまた、自動的に 19850ユーロ によります。</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Generated ids: tensor([[60715,    87,    56,    36,    25, 18064, 17517,    19, 12951,   111,\n",
      "          4982,    70,    19,    36,    32,  9896,   318,   150,    11,  9081,\n",
      "          1852, 34575,    17,     2,     0]], device='cuda:0')\n",
      "Decoded sentence: You have it in manual transmission and trim style... and it is automatically made up of 19850 euros.\n",
      "BLEU score: {'bleu': 0.25376192011637994, 'precisions': [0.5909090909090909, 0.3333333333333333, 0.2, 0.10526315789473684], 'brevity_penalty': 1.0, 'length_ratio': 1.1578947368421053, 'translation_length': 22, 'reference_length': 19}\n",
      "CHRF score: {'score': 58.78769967245788, 'char_order': 6, 'word_order': 0, 'beta': 2}\n"
     ]
    }
   ],
   "source": [
    "z = {'en': 'You have it in Style trim with manual transmission ... and Also automatically by 19,850 €.',\n",
    " 'jp': 'あなたはマニュアルトランスミッションとトリムスタイルでそれを持っている...とまた、自動的に19850ユーロによります。'} # 385\n",
    "translated = translate_text_print(model, tokenizer, z['jp'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
